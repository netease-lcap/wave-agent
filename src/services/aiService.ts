import type { ChatCompletionMessageParam, ModelId } from "../types/common";
import { VALID_MODEL_IDS, DEFAULT_MODEL_ID } from "../types/common";
import { toolRegistry } from "../plugins/tools";
import { logger } from "@/utils/logger";
import OpenAI from "openai";
import { ChatCompletionMessageToolCall } from "openai/resources";

// Initialize OpenAI client with environment variables
const openai = new OpenAI({
  apiKey: process.env.AIGW_TOKEN,
  baseURL: process.env.AIGW_URL,
});

// éªŒè¯æ¨¡å‹IDæ˜¯å¦æœ‰æ•ˆ
function isValidModelId(modelId: string): modelId is ModelId {
  return VALID_MODEL_IDS.includes(modelId as ModelId);
}

export interface CallAgentOptions {
  messages: ChatCompletionMessageParam[];
  sessionId?: string;
  abortSignal?: AbortSignal;
}

export interface CallAgentResult {
  content?: string;
  tool_calls?: ChatCompletionMessageToolCall[];
  usage?: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}

export interface GenerateCommitMessageOptions {
  diff: string;
  abortSignal?: AbortSignal;
}

export interface ApplyEditOptions {
  targetFile: string;
  instructions: string;
  codeEdit: string;
  abortSignal?: AbortSignal;
}

export async function applyEdit(options: ApplyEditOptions): Promise<string> {
  const { targetFile, instructions, codeEdit, abortSignal } = options;

  // å¦‚æœ targetFile ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œè¯´æ˜æ˜¯æ–°å»ºæ–‡ä»¶ï¼Œç›´æ¥è¿”å› codeEdit
  if (!targetFile || targetFile.trim() === "") {
    return codeEdit;
  }

  // ä½¿ç”¨ OpenAI æ¥å¤„ç†æ–‡ä»¶ç¼–è¾‘
  try {
    const response = await openai.chat.completions.create(
      {
        model: "gemini-2.5-flash",
        messages: [
          {
            role: "system",
            content: `You are a code editor. Apply the given instructions to edit the code. Return only the edited content without any explanations.`,
          },
          {
            role: "user",
            content: `File: ${targetFile}\n\nInstructions: ${instructions}\n\nCode to edit:\n${codeEdit}`,
          },
        ],
        temperature: 0.1,
      },
      {
        signal: abortSignal,
      },
    );

    return response.choices[0]?.message?.content || codeEdit;
  } catch (error) {
    if ((error as Error).name === "AbortError") {
      throw new Error("è¯·æ±‚å·²è¢«ä¸­æ–­");
    }
    logger.error("Failed to apply edit:", error);
    return codeEdit; // è¿”å›åŸå§‹å†…å®¹ä½œä¸ºåå¤‡
  }
}

export async function generateCommitMessage(
  options: GenerateCommitMessageOptions,
): Promise<string> {
  const { diff, abortSignal } = options;

  try {
    const response = await openai.chat.completions.create(
      {
        model: "gemini-2.5-flash",
        messages: [
          {
            role: "system",
            content: `Generate a concise commit message for the following git diff. The commit message should be in conventional commit format and under 72 characters.`,
          },
          {
            role: "user",
            content: diff,
          },
        ],
        temperature: 0.3,
        max_tokens: 100,
      },
      {
        signal: abortSignal,
      },
    );

    return response.choices[0]?.message?.content?.trim() || "Update files";
  } catch (error) {
    if ((error as Error).name === "AbortError") {
      throw new Error("è¯·æ±‚å·²è¢«ä¸­æ–­");
    }
    logger.error("Failed to generate commit message:", error);
    return "Update files";
  }
}

export async function callAgent(
  options: CallAgentOptions,
): Promise<CallAgentResult> {
  const { messages, abortSignal } = options;

  // è·å–æ¨¡å‹é…ç½®
  const envModel = process.env.AIGW_MODEL;
  const modelId: ModelId =
    envModel && isValidModelId(envModel)
      ? (envModel as ModelId)
      : DEFAULT_MODEL_ID;

  try {
    // æ·»åŠ ç³»ç»Ÿæç¤ºè¯
    const systemMessage: OpenAI.Chat.Completions.ChatCompletionMessageParam = {
      role: "system",
      content: `You are a professional web development expert.

## Current Project TODOs

*TODOs will be generated based on user instructions and project requirements. Use markdown checkboxes to track completion status.*

### ğŸ”„ In Progress
*Tasks currently being worked on*

### âœ… Completed
*Completed tasks will be listed here*

## Tool Usage Guidelines:

- Check that all required parameters for each tool call are provided or can reasonably be inferred from context
- If there are missing values for required parameters, ask the user to supply these values before proceeding
- If the user provides a specific value for a parameter (especially in quotes), use that value EXACTLY
- DO NOT make up values for or ask about optional parameters
- Carefully analyze descriptive terms in the request as they may indicate required parameter values

## TODO Management:

- Always reference the current TODOs when starting new tasks
- Update TODO status using markdown checkboxes: \`- [ ]\` for pending, \`- [x]\` for completed
- Add new TODOs as they arise during development
- Use emojis and clear categorization (ğŸ”„ In Progress, ğŸ“‹ Pending, âœ… Completed)
- Provide context in TODO descriptions to help future development

Remember: Always plan your approach first, explain it clearly, then execute systematically while maintaining TODO awareness.`,
    };

    // ChatCompletionMessageParam[] å·²ç»æ˜¯ OpenAI æ ¼å¼ï¼Œæ·»åŠ ç³»ç»Ÿæç¤ºè¯åˆ°å¼€å¤´
    const openaiMessages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] =
      [systemMessage, ...messages];

    // è·å–å·¥å…·é…ç½®
    const tools = toolRegistry.getToolsConfig();

    // è°ƒç”¨ OpenAI APIï¼ˆéæµå¼ï¼‰
    const response = await openai.chat.completions.create(
      {
        model: modelId,
        messages: openaiMessages,
        tools,
        temperature: 0,
        max_completion_tokens: 8192,
      },
      {
        signal: abortSignal,
      },
    );

    const choice = response.choices[0];
    if (!choice) {
      throw new Error("No response from OpenAI");
    }

    const result: CallAgentResult = {};

    // è¿”å›å†…å®¹
    if (choice.message.content) {
      result.content = choice.message.content;
    }

    // è¿”å›å·¥å…·è°ƒç”¨
    if (choice.message.tool_calls) {
      result.tool_calls = choice.message.tool_calls;
    }

    // è¿”å› token ä½¿ç”¨ä¿¡æ¯
    if (response.usage) {
      result.usage = {
        prompt_tokens: response.usage.prompt_tokens,
        completion_tokens: response.usage.completion_tokens,
        total_tokens: response.usage.total_tokens,
      };
    }

    return result;
  } catch (error) {
    if ((error as Error).name === "AbortError") {
      throw new Error("è¯·æ±‚å·²è¢«ä¸­æ–­");
    }
    logger.error("Failed to call OpenAI:", error);
    throw error;
  }
}

export interface CompressMessagesOptions {
  messages: ChatCompletionMessageParam[];
  abortSignal?: AbortSignal;
}

export async function compressMessages(
  options: CompressMessagesOptions,
): Promise<string> {
  const { messages, abortSignal } = options;

  try {
    const response = await openai.chat.completions.create(
      {
        model: "gemini-2.5-flash",
        messages: [
          {
            role: "system",
            content: `ä½ æ˜¯ä¸€ä¸ªå¯¹è¯å†å²å‹ç¼©ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹å¯¹è¯å†å²å‹ç¼©ä¸ºç®€æ´ä½†åŒ…å«å…³é”®ä¿¡æ¯çš„æ‘˜è¦ã€‚
è¦æ±‚ï¼š
1. ä¿ç•™é‡è¦çš„æŠ€æœ¯è®¨è®ºè¦ç‚¹
2. ä¿ç•™å…³é”®çš„æ–‡ä»¶æ“ä½œå’Œä»£ç ä¿®æ”¹ä¿¡æ¯
3. ä¿ç•™ç”¨æˆ·çš„ä¸»è¦éœ€æ±‚å’ŒåŠ©æ‰‹çš„ä¸»è¦è§£å†³æ–¹æ¡ˆ
4. ä½¿ç”¨ç¬¬ä¸‰äººç§°æ€»ç»“æ ¼å¼
5. æ§åˆ¶åœ¨300å­—ä»¥å†…
6. ç”¨ä¸­æ–‡å›å¤`,
          },
          {
            role: "user",
            content: `è¯·å‹ç¼©ä»¥ä¸‹å¯¹è¯å†å²ï¼š`,
          },
          ...messages,
        ],
        temperature: 0.1,
        max_tokens: 500,
      },
      {
        signal: abortSignal,
      },
    );

    return response.choices[0]?.message?.content?.trim() || "å¯¹è¯å†å²å‹ç¼©å¤±è´¥";
  } catch (error) {
    if ((error as Error).name === "AbortError") {
      throw new Error("å‹ç¼©è¯·æ±‚å·²è¢«ä¸­æ–­");
    }
    logger.error("Failed to compress messages:", error);
    return "å¯¹è¯å†å²å‹ç¼©å¤±è´¥";
  }
}
